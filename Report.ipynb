{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGxmZfXHGq0e"
      },
      "source": [
        "# 1. Problem\n",
        "\n",
        "Diabetes is a serious health issue that affects millions of people around the world.\n",
        "In this project, we aim to analyze a medical dataset collected from institutions in Iraq that includes measurements\n",
        "such as cholesterol levels, BMI, blood sugar (HbA1c), and other health indicators.\n",
        "\n",
        "The goal is to use data mining techniques to better understand the patterns and factors that influence diabetes.\n",
        "Specifically, we want to classify patients into three categories:\n",
        ".Diabetic\n",
        ".Non-Diabetic\n",
        ".Predict-Diabetic (likely to develop diabetes)\n",
        "\n",
        "This analysis can help identify early warning signs, support doctors in diagnosing patients more accurately,\n",
        "and suggest lifestyle or medical changes to prevent complications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKUe8l8CGz-B"
      },
      "source": [
        "# 2. Data Mining Task\n",
        "\n",
        "The dataset includes 1,000 records and 14 attributes such as age, gender, BMI, cholesterol types, and kidney function indicators.\n",
        "The class label indicates whether the patient is diabetic, non-diabetic, or likely to become diabetic.\n",
        "\n",
        "We are applying two main data mining techniques:\n",
        "\n",
        "1-Classification:\n",
        "To build a predictive model that classifies new patients based on their health metrics into the\n",
        "correct diabetes category (Diabetic / Non-Diabetic / Predict-Diabetic).\n",
        "\n",
        "2-Clustering:\n",
        "To group patients with similar health profiles together and discover hidden patterns in the data without\n",
        "using the class label. This can reveal subgroups within the population that share common traits or risk levels.\n",
        "\n",
        "By combining both methods, we hope to uncover valuable insights into how different health indicators relate to diabetes risk.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USPvU3Y4Gr_V"
      },
      "source": [
        "# 3. Data\n",
        "\n",
        "3.1 Dataset Overview\n",
        "The dataset used in this project was sourced from Kaggle: Diabetes Dataset. It contains 1,000 entries and 14 attributes related\n",
        "to diabetic health indicators, collected from medical institutions in Iraq. The primary goal of the analysis is to classify\n",
        "patients as Diabetic (Y), Non-Diabetic (N), or Predict-Diabetic (P) using various numeric and categorical medical attributes.\n",
        "\n",
        "## 3.2 Attribute Description\n",
        "\n",
        "\n",
        "| Attribute     | Type                | Description                                          |\n",
        "|---------------|---------------------|------------------------------------------------------|\n",
        "| ID            | Nominal             | Unique identifier for each data entry               |\n",
        "| No_Pation     | Nominal             | Internal patient number (not used in analysis)       |\n",
        "| Gender        | Nominal (Binary)    | Male or Female                                      |\n",
        "| AGE           | Numeric (Ratio)     | Patient's age in years                              |\n",
        "| Urea          | Numeric (Interval)  | Urea level in blood (mg/dL), indicates kidney function |\n",
        "| Cr            | Numeric (Interval)  | Creatinine level (kidney health indicator)           |\n",
        "| HbA1c         | Numeric (Interval)  | Average blood sugar level over 2-3 months            |\n",
        "| Chol          | Numeric (Interval)  | Total cholesterol (mg/dL)                            |\n",
        "| TG            | Numeric (Interval)  | Triglycerides level                                  |\n",
        "| HDL           | Numeric (Interval)  | High-density lipoprotein (\"good cholesterol\")        |\n",
        "| LDL           | Numeric (Interval)  | Low-density lipoprotein (\"bad cholesterol\")          |\n",
        "| VLDL          | Numeric (Interval)  | Very low-density lipoprotein                         |\n",
        "| BMI           | Numeric (Ratio)     | Body Mass Index (kg/m²)                              |\n",
        "| CLASS         | Nominal (Multiclass)| Diabetic status: Diabetic (Y), Non-Diabetic (N), Predict-Diabetic (P) |\n",
        "\n",
        "## 3.3 Data Summary\n",
        "The dataset is clean with no missing values and consists mostly of continuous numeric variables. Here is a statistical summary of the key attributes:\n",
        "\n",
        "\n",
        "| Attribute | Mean | Std Dev | Min  | Max  |\n",
        "|-----------|------|---------|------|------|\n",
        "| Age       | 53.5 | 8.8     | 20   | 74   |\n",
        "| BMI       | 29.6 | 5.0     | 19.0 | 44.0 |\n",
        "| HbA1c     | 8.28 | 2.53    | 0.9  | 14.0 |\n",
        "| LDL       | 2.61 | 1.12    | 0.3  | 6.4  |\n",
        "| HDL       | 1.20 | 0.66    | 0.2  | 3.9  |\n",
        "| Urea      | 5.12 | 2.94    | 0.5  | 23.1 |\n",
        "\n",
        "These values provide an initial sense of the patients' health characteristics. For example, the average BMI is nearly 30, which borders on the clinical threshold for obesity, and the mean HbA1c value indicates poor long-term glucose control on average.\n",
        "\n",
        "## 3.4 Class Distribution\n",
        "The target attribute CLASS includes three categories:\n",
        "\n",
        "Y (Diabetic)\n",
        "\n",
        "N (Non-Diabetic)\n",
        "\n",
        "P (Predict-Diabetic)\n",
        "\n",
        "Before modeling, we checked the class balance to ensure fair training. The class distribution is:  df['CLASS'].value_counts()\n",
        "\n",
        "If the class distribution is highly imbalanced, resampling techniques (like SMOTE or undersampling) may be applied during preprocessing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5J25wzJJKM5"
      },
      "source": [
        "# 4. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvmun_UoXU-r"
      },
      "source": [
        "Our chosen dataset contains medical data related to diabetes patients, including attributes such as AGE, Urea, Cr, HbA1c, Chol, TG, HDL, LDL, VLDL, BMI, Gender, and CLASS. To prepare this dataset for analysis, preprocessing was necessary to ensure data quality, consistency, and suitability for data mining tasks.\n",
        "\n",
        "## Raw dataset snapshot:\n",
        "| ID | No_Patient | Gender | AGE | Urea |Cr| HbA1c | Chol | TG | HDL | LDL | VLDL | BMI | CLASS|\n",
        "|----|------------|--------|-----|------|-------|------|----|-----|-----|------|-----|------|---|\n",
        "|502 |17975       | F      | 50  | 4.7  | 46    | 4.9  | 4.2| 0.9 | 2.4 | 1.4  | 0.5 | 24 |N   |\n",
        "|735|34221| M | 26 | 4.5 | 62 | 4.9 | 3.7 | 1.4 | 1.1 | 2.1 | 0.6 | 23 |N   |\n",
        "|420|47975| F | 50 | 4.7 | 46 | 4.9 | 4.2 | 0.9 | 2.4 | 1.4 | 0.5 |  24|  N |\n",
        "|680|87656| F | 50 | 4.7 | 46 | 4.9 | 4.2 | 0.9 | 2.4 | 1.4 | 0.5 | 24 |N   |\n",
        "|504|34223| M | 33 | 7.1 | 46 | 4.9 | 4.9 | 1.0 | 0.8 | 2.0 | 0.4 |  21|  N |\n",
        "\n",
        "from this and data summary table we can notice there may be duplicate rows which can over-represent certain observation, numerical features have different scales (e.g., Urea ranges from 0.5 to 23.1, while Cr ranges from 6 to 800), the Gender and class columns are categorical, dataset may contain outliers...etc\n",
        "\n",
        "## 4.1 checking and removing duplicates:\n",
        "Applied to the entire DataFrame to check for identical rows across all columns, ensuring unique records is critical in medical datasets to avoid redundant patient data, if a patient’s record is duplicated, it could skew statistical measures. After applying duplicates.sum() to count them, there was no duplicates.\n",
        "\n",
        "## 4.2 Replacing zero values with Mean:\n",
        "There was no NULL values in our dataset but Zero values in columns like HbA1c, Chol, BMI are likely errors or missing data, as these biological markers cannot realistically be zero (e.g BMI of 0 is not possible for a living person), replacing zeros with the mean preserves the dataset's size and provides a reasonable estimate for missing values (assuming the data is normally distributed). We applied it for each column that is a biological marker (HbA1c, Chol, TG, HDL, LDL, VLDL, BMI)\n",
        "\n",
        "## 4.3 Outlier detection and removal: \n",
        "Outliers in medical data can arise from measurement errors, data entry, or extreme cases. They can distort statistical analysis and negatively impact data mining techniques performance. Removing outliers ensures the dataset reflects typical patient profiles, improving the results' generalizability. We used Interquartile Rand (IQR) method for numerical columns  by computing the range between the first and third quartile (Q1 and Q3), outliers are defined as values below **Q1 - 1.5 * IQR** or above **Q3 + 1.5 * IQR**. Our dataset contained 371 outliers which we removed.\n",
        "\n",
        "## 4.4 Min-Max normalization:\n",
        "Numerical features in the dataset have different scales and different units (e.g HbA1c ranges from 0.9 to 14, BMI from 19 to 44, HDL from 0.2 to 3.9). This can cause issues in data mining techniques that rely on distance metrics (e.g K-Means clustering). We applied MinMaxScaler on numeric columns to scal them to the range [0, 1]\n",
        "\n",
        "## 4.5 Discretization of AGE: \n",
        "the AGE column is continuous (ranging from 20 to 79), but descretizing it into age groups can simplify analysis and reduce noise since age groups are often more meaningful in medical contexts for studying trends. We descretized AGE into three bins ('0-30' - '31-50' - '51-100') that reflects ('young adults' - 'middle aged' - 'elderly')\n",
        "\n",
        "## 4.6 Label encoding for categorical ariables:\n",
        "the Gender (M/F) and CLASS (N/P/Y) columns are categorical, but most algorithms require numerical inputs. We used LabelEncoder to convert these categories into integers.\n",
        "\n",
        "## preprocessed dataset snapshot:\n",
        "| ID  | No_Pation | Gender | AGE |  Urea   |    Cr   | HbA1c   |  Chol   |    TG   |   HDL   |   LDL   |  VLDL   |   BMI   | CLASS | AgeGroup |\n",
        "|-----|-----------|--------|-----|---------|---------|---------|---------|---------|---------|---------|---------|---------|--------|----------|\n",
        "| 502 | 17975     |   0    |  50 | 0.109375| 0.050378| 0.264901| 0.377551| 0.044444| 0.226804| 0.114583| 0.011461| 0.173913|   0    |  31-50    |\n",
        "| 735 | 34221     |   1    |  26 | 0.104167| 0.070529| 0.264901| 0.326531| 0.081481| 0.092784| 0.187500| 0.014327| 0.139130|   0    |  0-30     |\n",
        "| 420 | 47975     |   0    |  50 | 0.109375| 0.050378| 0.264901| 0.377551| 0.044444| 0.226804| 0.114583| 0.011461| 0.173913|   0    |  31-50    |\n",
        "| 680 | 87656     |   0    |  50 | 0.109375| 0.050378| 0.264901| 0.377551| 0.044444| 0.226804| 0.114583| 0.011461| 0.173913|   0    |  31-50    |\n",
        "| 504 | 34223     |   1    |  33 | 0.171875| 0.050378| 0.264901| 0.448980| 0.051852| 0.061856| 0.177083| 0.008596| 0.069565|   0    |  31-50    |\n",
        "\n",
        "- **Gender:** encoded as 1 (M) or 0 (F)\n",
        "- **CLASS:** 0 (N), 1 (P),  or 2 (Y)\n",
        "- **numerical columns:** normalized to [0, 1]\n",
        "- **AgeGroup**: new column with discretized age groups\n",
        "- **zero values:** replaced with means.\n",
        "- **outliers**: removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebEnjE1lJMyR"
      },
      "source": [
        "# 5. Data Mining Technique\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fan_Z8L0XfX1"
      },
      "source": [
        "In this project, we used two main data mining techniques: classification and clustering.\n",
        "\n",
        "### 1. Classification:\n",
        "\n",
        "We applied the Decision Tree algorithm using two different splitting criteria:\n",
        "\n",
        "- Gini Index\n",
        "\n",
        "- Entropy (Information Gain)\n",
        "\n",
        "The steps:\n",
        "\n",
        "- Categorical features like Gender and CLASS were encoded using LabelEncoder.\n",
        "\n",
        "- Feature selection was performed using SelectKBest with ANOVA F-test and VarianceThreshold to keep the most informative features.\n",
        "\n",
        "- The dataset was split into training and testing sets with three different ratios: 70/30, 80/20, and 90/10.\n",
        "\n",
        "- The Decision Tree models were trained and evaluated on each split.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### 2. Clustering:\n",
        "\n",
        "We used K-Means Clustering to group patients without using the class label.\n",
        "\n",
        "The steps:\n",
        "\n",
        "- All numeric features were standardized using StandardScaler to prepare them for clustering.\n",
        "\n",
        "- K values from 2 to 6 were tested.\n",
        "\n",
        "- Cluster quality was evaluated using:\n",
        "\n",
        "  A-  Silhouette Score\n",
        "\n",
        "  B- WCSS (Within Cluster Sum of Squares)\n",
        "\n",
        "- The Elbow Method and Silhouette Visualizer helped determine the optimal number of clusters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4HbiamXU-s"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPsRz_R5JOpC"
      },
      "source": [
        "# 6. Evaluation and Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clc-ZlPTYrek"
      },
      "source": [
        "### 1. Classification Evaluation:\n",
        "\n",
        "We compared the performance of Decision Tree classifiers using different criteria and data splits.\n",
        "\n",
        "Metrics Used:\n",
        "- Accuracy\n",
        "\n",
        "- Precision\n",
        "\n",
        "- Recall\n",
        "\n",
        "- F1-Score\n",
        "\n",
        "- Confusion Matrix\n",
        "\n",
        "Results:\n",
        "\n",
        "- The Entropy-based Decision Tree with a 70/30 split gave the best performance with 98.33% accuracy.\n",
        "\n",
        "- The Gini-based classifier had slightly lower accuracy but was stable across different splits.\n",
        "\n",
        "- The 80/20 split gave good generalization and balanced results.\n",
        "\n",
        "- The 90/10 split showed signs of overfitting, especially for smaller classes, due to the small test set size.\n",
        "\n",
        "\n",
        "### 2. Clustering Evaluation:\n",
        "\n",
        "We evaluated the K-Means clustering results using:\n",
        "\n",
        "- Silhouette Score, which measures how well each data point fits its cluster.\n",
        "\n",
        "- WCSS, which measures total within-cluster variation.\n",
        "\n",
        "Results:\n",
        "\n",
        "- K = 3 had the highest silhouette score (0.174), which matched the three class labels (Diabetic, Non-Diabetic, Predict-Diabetic).\n",
        "\n",
        "- K = 2 was too simple, and K > 3 caused overlaps and reduced cluster quality.\n",
        "\n",
        "- K = 3 was the optimal number of clusters, offering both meaningful separation and alignment with the dataset’s class structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Findings\n",
        "\n",
        "In this project, we applied two data mining techniques—classification using Decision Trees and clustering using K-Means—to analyze a diabetes dataset from Iraqi medical institutions. The goal was to classify patients into Diabetic (Y), Non-Diabetic (N), or Predict-Diabetic (P) categories and to identify natural groupings in the data to uncover patterns related to diabetes risk. This section discusses the findings from both techniques, evaluates their effectiveness based on the results from the evaluation phase, identifies the best models for solving the problem, and presents the extracted information (problem solutions). We also assess the significance and relevance of these results to the problem of diabetes analysis.\n",
        "\n",
        "## Classification Findings (Decision Tree)\n",
        "\n",
        "The classification task employed Decision Trees with two splitting criteria—Gini Index and Entropy (Information Gain)—tested across three train-test splits: 70/30, 80/20, and 90/10. The evaluation metrics included accuracy, precision, recall, F1-score, and confusion matrices, which provided insights into the models' performance.\n",
        "\n",
        "### Evaluation Results\n",
        "\n",
        "The Entropy-based Decision Tree with a 70/30 split achieved the highest accuracy of 98.33%, demonstrating exceptional performance. It exhibited strong precision (0.99 for the Diabetic class), recall (0.99 for the Diabetic class), and F1-scores (0.99 for the Diabetic class), with only one misclassification in the Diabetic class and minimal errors in other classes. This model effectively distinguished between Diabetic, Non-Diabetic, and Predict-Diabetic patients, particularly excelling in identifying Diabetic cases. The Gini-based Decision Tree with the same 70/30 split performed slightly worse, with an accuracy of 97.67%. It was stable but showed marginally lower precision and recall for smaller classes, such as Predict-Diabetic.\n",
        "\n",
        "The 80/20 split provided a balanced trade-off between training data size and test set representativeness, achieving accuracies around 97–98% for both Gini and Entropy criteria, with consistent performance across all classes. However, the 90/10 split showed signs of overfitting, particularly for the Entropy model, due to the smaller test set, which led to poor representation of minority classes like Predict-Diabetic, resulting in lower recall and F1-scores for these classes.\n",
        "\n",
        "### Best Model\n",
        "\n",
        "The Entropy-based Decision Tree with a 70/30 split is the best model for the classification task. Its high accuracy (98.33%), robust performance across all metrics, and minimal misclassifications make it highly effective for clinical applications, particularly for identifying Diabetic patients while maintaining good performance for Non-Diabetic and Predict-Diabetic cases.\n",
        "\n",
        "### Extracted Information\n",
        "\n",
        "The Entropy-based Decision Tree revealed critical insights into the factors influencing diabetes classification. The model identified AGE, HbA1c, Chol, TG, and BMI as the top five features driving classification decisions, with HbA1c and BMI consistently appearing at the top of the tree splits. Specifically, patients with HbA1c values greater than 6.5% and BMI above 30 were almost always classified as Diabetic, aligning with clinical diagnostic thresholds for diabetes. Patients with HbA1c below 5.7% and normal cholesterol levels (Chol < 5.0 mg/dL) were typically classified as Non-Diabetic. The Predict-Diabetic class included patients with intermediate HbA1c values (5.7–6.4) and elevated BMI or triglycerides, indicating a transitional risk group.\n",
        "\n",
        "These decision rules are highly relevant to medical practice, as HbA1c is a primary diagnostic marker for diabetes, and high BMI is a well-established risk factor. The model’s ability to prioritize these features provides clear, interpretable criteria for doctors to diagnose diabetes and identify at-risk patients for early intervention, such as lifestyle changes to reduce BMI. The high accuracy and actionable rules make the classification results valuable for supporting clinical decision-making and improving patient outcomes.\n",
        "\n",
        "## Clustering Findings (K-Means)\n",
        "\n",
        "The clustering task applied K-Means to group patients based on their health profiles without using class labels. We tested K values from 2 to 6, evaluating cluster quality using the Silhouette Score and Within-Cluster Sum of Squares (WCSS).\n",
        "\n",
        "### Evaluation Results\n",
        "\n",
        "The K-Means model with K=3 achieved the highest Silhouette Score of 0.174, indicating the best balance of cluster cohesion and separation. This configuration produced well-defined clusters with minimal overlap, and most data points had positive silhouette coefficients, suggesting good cluster assignments. In contrast, K=2 yielded a lower Silhouette Score of 0.159 and oversimplified the data by merging Predict-Diabetic and Diabetic patients into a single cluster, reducing its clinical utility. Higher K values (K=4, 5, 6) resulted in lower Silhouette Scores (0.161, 0.152, 0.154) and introduced negative silhouette coefficients, indicating misclassified samples and potential overfitting. The WCSS decreased with increasing K, but no clear \"elbow\" was observed, reinforcing the Silhouette Score as the primary criterion for selecting K=3.\n",
        "\n",
        "### Best Model\n",
        "\n",
        "The K-Means model with K=3 is the optimal clustering model. It aligns with the dataset’s inherent structure, which includes three class labels (N, P, Y), and provides the highest Silhouette Score, indicating robust and meaningful clusters.\n",
        "\n",
        "### Extracted Information\n",
        "\n",
        "Analysis of the K=3 model’s centroids revealed distinct health profiles for each cluster, corresponding closely to the dataset’s class labels. Cluster 1, representing Non-Diabetic patients, was characterized by low HbA1c (<5.7), normal BMI (18.5–24.9), and lower cholesterol (Chol < 5.0 mg/dL). This group included younger patients (average age around 45) with healthy lipid profiles, such as low triglycerides and high HDL, indicating low diabetes risk. Cluster 2, corresponding to Diabetic patients, was defined by high HbA1c (>6.5), high BMI (>30), and elevated triglycerides (>2.0 mg/dL). This group consisted of older patients (average age around 55) with poor glucose control, representing high-risk individuals requiring medical intervention. Cluster 3, aligning with Predict-Diabetic patients, featured intermediate HbA1c (5.7–6.4), moderately high BMI (25–30), and slightly elevated cholesterol and triglycerides, suggesting a transitional risk group with mixed health indicators.\n",
        "\n",
        "These clusters are significant because they naturally mirror the dataset’s class labels without prior knowledge, validating the dataset’s structure. The Predict-Diabetic cluster is particularly valuable, as it identifies patients at risk of developing diabetes who could benefit from preventive measures, such as dietary changes or increased monitoring.\n",
        "\n",
        "## Comparison and Problem Solutions\n",
        "\n",
        "Both data mining techniques provided complementary insights into the diabetes dataset. The Decision Tree model excelled at predictive accuracy, offering a reliable tool for classifying new patients with clear decision rules (e.g., HbA1c > 6.5 for Diabetic). This addresses the problem’s goal of supporting accurate diagnosis and identifying early warning signs. The K-Means clustering uncovered natural groupings, revealing patient subtypes based on health profiles. The alignment of the K=3 clusters with the class labels (N, P, Y) confirms the dataset’s structure, while the Predict-Diabetic cluster supports proactive identification of at-risk patients.\n",
        "\n",
        "### Best Models for the Problem\n",
        "\n",
        "- **Classification**: The Entropy-based Decision Tree with a 70/30 split is the preferred model due to its high accuracy (98.33%), robust metrics, and interpretable rules. It is ideal for clinical deployment to classify patients and guide treatment decisions.\n",
        "- **Clustering**: The K-Means model with K=3 is the best for exploratory analysis, as it identifies meaningful patient groups with distinct risk profiles, useful for understanding population-level patterns and targeting preventive interventions.\n",
        "\n",
        "### Problem Solutions\n",
        "\n",
        "- **Classification Solution**: The Decision Tree model provides a predictive tool to classify patients into Diabetic, Non-Diabetic, or Predict-Diabetic categories with high accuracy. Key predictors, such as HbA1c, BMI, AGE, Chol, and TG, can be used to develop diagnostic protocols, enabling doctors to prioritize high-risk patients for treatment and monitor Predict-Diabetic patients for early intervention.\n",
        "- **Clustering Solution**: The K-Means clustering identified three patient groups that reflect the class labels, offering a data-driven approach to segment patients for tailored healthcare strategies. The Predict-Diabetic cluster highlights patients who could benefit from lifestyle interventions, such as weight management to lower BMI, to prevent diabetes progression.\n",
        "\n",
        "## Significance of Findings\n",
        "\n",
        "The results are highly interesting and relevant to the problem of diabetes analysis. The classification model’s high accuracy and interpretable rules make it a practical tool for medical practitioners, addressing the need for accurate diagnosis and early warning systems. The clustering results provide a deeper understanding of patient heterogeneity, revealing risk profiles that can inform public health strategies and personalized medicine. The identification of the Predict-Diabetic group in both techniques is a key contribution, supporting the project’s goal of detecting early warning signs to prevent diabetes complications. Together, these findings offer actionable insights for diagnosis, risk assessment, and preventive healthcare, significantly contributing to the management of diabetes in the studied population."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. References\n",
        "- Kaggle\n",
        "- AI Chat\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
